1
00:00:00,610 --> 00:00:02,920
Let's talk a little bit more about a specific example

2
00:00:02,920 --> 00:00:06,430
of supervised learning. We might want to ask the question, can we

3
00:00:06,430 --> 00:00:08,990
write an equation that takes as input a bunch of

4
00:00:08,990 --> 00:00:14,050
information. For example, height, weight, birth year or position about baseball

5
00:00:14,050 --> 00:00:17,540
players and predicts their lifetime number of home runs. To

6
00:00:17,540 --> 00:00:19,670
do this, we want to write an algorithm that does the

7
00:00:19,670 --> 00:00:22,340
following. Takes in the data points for which we have all

8
00:00:22,340 --> 00:00:26,260
these input attributes, and the player's lifetime number of home runs.

9
00:00:26,260 --> 00:00:29,100
Then builds the most accurate equation to predict lifetime number of

10
00:00:29,100 --> 00:00:32,960
home runs, using these input variables. We can then use this equation

11
00:00:32,960 --> 00:00:35,680
to predict the lifetime number of home runs for players for

12
00:00:35,680 --> 00:00:38,620
whom we did not initially have number of home runs. But we

13
00:00:38,620 --> 00:00:41,300
do have all the other data. So, their height and weight

14
00:00:41,300 --> 00:00:44,450
and birth year and position maybe. One way that we might be

15
00:00:44,450 --> 00:00:47,650
able to solve this problem is using linear regression and one

16
00:00:47,650 --> 00:00:49,580
basic implementation on machine learning is

17
00:00:49,580 --> 00:00:51,570
to perform linear regression by using

18
00:00:51,570 --> 00:00:55,225
gradient descent. What is this? How does this work? Lets learn about

19
00:00:55,225 --> 00:00:57,205
gradient descent by working through an

20
00:00:57,205 --> 00:00:59,550
example that utilizes our baseball data set.
