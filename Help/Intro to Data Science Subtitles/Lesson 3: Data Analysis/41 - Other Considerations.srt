1
00:00:00,940 --> 00:00:02,960
What we've covered so far in this class is

2
00:00:02,960 --> 00:00:07,250
a very simplistic and a very partial coverage of

3
00:00:07,250 --> 00:00:10,790
linear regression with gradient decent. If you were ever

4
00:00:10,790 --> 00:00:12,980
to try and use linear regression to solve a real

5
00:00:12,980 --> 00:00:16,190
world problem, there are a lot of additional considerations

6
00:00:16,190 --> 00:00:18,170
that you would want to think about in order

7
00:00:18,170 --> 00:00:22,205
to seriously implement linear regression with or without gradient

8
00:00:22,205 --> 00:00:26,210
descent. First of all, gradient descent is only one implementation

9
00:00:26,210 --> 00:00:28,440
of linear regression. There are a bunch of other

10
00:00:28,440 --> 00:00:31,280
ones, and in some sense, they may be better.

11
00:00:31,280 --> 00:00:34,060
Ordinary Least Squares for example, is always guaranteed to

12
00:00:34,060 --> 00:00:38,030
find the optimal solution when performing linear regression, whereas gradient

13
00:00:38,030 --> 00:00:42,050
descent is not. Additionally, we haven't really talked about

14
00:00:42,050 --> 00:00:46,490
parameter estimation and putting confidence intervals on those parameters. In

15
00:00:46,490 --> 00:00:48,880
the models that we've built, we've given exact values

16
00:00:48,880 --> 00:00:51,400
for all of our thetas. But as you can imagine,

17
00:00:51,400 --> 00:00:54,220
there's some confidence that we have in those values.

18
00:00:54,220 --> 00:00:57,840
You can imagine doing more thorough statistical analysis in saying

19
00:00:57,840 --> 00:01:01,070
what are the confidence intervals on these parameters? And

20
00:01:01,070 --> 00:01:04,069
we could answer questions like what is the likelihood that

21
00:01:04,069 --> 00:01:06,230
I would get this value for this parameter if

22
00:01:06,230 --> 00:01:08,969
this parameter actually had no effect on our output variable?

23
00:01:10,500 --> 00:01:13,430
We also might worry about issues like over or under

24
00:01:13,430 --> 00:01:16,920
fitting. This isn't so much a problem with linear regression,

25
00:01:16,920 --> 00:01:19,905
but with more complicated models, you might expect our model to

26
00:01:19,905 --> 00:01:23,350
over-fit. Say we had some data like these red points that

27
00:01:23,350 --> 00:01:26,240
was approximately linear. If we had a model with many different

28
00:01:26,240 --> 00:01:29,250
parameters, we might be able to fit the data points exactly

29
00:01:29,250 --> 00:01:33,740
whereas the actual underlying model is this black line. One way

30
00:01:33,740 --> 00:01:36,110
to deal with this is usually to split our data into

31
00:01:36,110 --> 00:01:38,940
a training set and a test set. Then we can train

32
00:01:38,940 --> 00:01:42,040
a model on the training data, and then test it on

33
00:01:42,040 --> 00:01:44,200
the test data, just as the name would imply. This

34
00:01:44,200 --> 00:01:48,610
is called cross validation. We might also see under-fitting, where we

35
00:01:48,610 --> 00:01:51,520
have data that's clearly nonlinear, but we only try and fit

36
00:01:51,520 --> 00:01:54,190
a linear model to it. This is also a problem that

37
00:01:54,190 --> 00:01:57,810
you might encounter. Another issue worth mentioning again is that our

38
00:01:57,810 --> 00:02:00,980
cost function may have numerous local minima. That means that when

39
00:02:00,980 --> 00:02:04,050
using gradient descent, our algorithm can become trapped in a local

40
00:02:04,050 --> 00:02:06,830
minimum that isn't actually the global minimum of the cost function.

41
00:02:08,100 --> 00:02:10,110
This isn't a problem of other implementations of linear

42
00:02:10,110 --> 00:02:13,230
regression, but when using gradient descent, this means that maybe

43
00:02:13,230 --> 00:02:17,690
we perform gradient descent numerous times, randomizing our initial values

44
00:02:17,690 --> 00:02:21,390
of theta with different random values every time. You may

45
00:02:21,390 --> 00:02:23,270
also already know this, but when using a random

46
00:02:23,270 --> 00:02:26,950
number generator to generate random values, it's important to seed

47
00:02:26,950 --> 00:02:29,790
your random values and to store that seed. That way,

48
00:02:29,790 --> 00:02:33,510
you have a result that's replicable and other researchers can

49
00:02:33,510 --> 00:02:36,460
look at your findings and also produce them on their own machines

50
00:02:36,460 --> 00:02:39,640
when they run the same script. This has been a whirlwind tour

51
00:02:39,640 --> 00:02:42,350
of a bunch of additional considerations that we didn't really touch in

52
00:02:42,350 --> 00:02:44,160
this lesson. These are all really

53
00:02:44,160 --> 00:02:47,270
important in machine learning, statistics, using

54
00:02:47,270 --> 00:02:50,160
linear regression in the real world. I really wanted to focus here

55
00:02:50,160 --> 00:02:53,550
on just implementing gradient descent and the basics of how it works.

56
00:02:53,550 --> 00:02:55,800
But if you really want to use any of these things to

57
00:02:55,800 --> 00:02:59,080
solve a real life problem, you'd want to think about local minima,

58
00:02:59,080 --> 00:03:02,770
you know other implementations of linear aggression, the confidence intervals

59
00:03:02,770 --> 00:03:06,370
on your parameters and if you're using a statistical modeling package

60
00:03:06,370 --> 00:03:09,650
in R or Python or Stata, they'll usually give you

61
00:03:09,650 --> 00:03:11,230
these confidence intervals. And use

62
00:03:11,230 --> 00:03:13,170
a reliable implementation of linear aggression.
